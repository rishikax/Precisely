{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleanedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['psyte_PSYTECategoryCode', 'psyte_PSYTEGroupCode', 'psyte_censusBlock',\n",
       "       'psyte_censusBlockGroup', 'psyte_censusBlockPopulation',\n",
       "       'psyte_censusBlockHouseholds', 'psyte_PSYTESegmentCode.description',\n",
       "       'psyte_householdIncomeVariable.value',\n",
       "       'psyte_householdIncomeVariable.description',\n",
       "       'psyte_propertyValueVariable.value',\n",
       "       'psyte_propertyValueVariable.description',\n",
       "       'psyte_propertyTenureVariable.value',\n",
       "       'psyte_propertyTenureVariable.description',\n",
       "       'psyte_propertyTypeVariable.value',\n",
       "       'psyte_propertyTypeVariable.description',\n",
       "       'psyte_urbanRuralVariable.value',\n",
       "       'psyte_urbanRuralVariable.description', 'coastal_preciselyID',\n",
       "       'coastal_waterbodyName', 'coastal_nearestWaterbodyCounty',\n",
       "       'coastal_nearestWaterbodyState', 'coastal_nearestWaterbodyAdjacentName',\n",
       "       'coastal_nearestWaterbodyAdjacentType',\n",
       "       'coastal_distanceToNearestCoastFeet',\n",
       "       'coastal_nearestWaterbodyType.value',\n",
       "       'coastal_nearestWaterbodyType.description', 'flood_preciselyID',\n",
       "       'flood_floodID', 'flood_femaMapPanelIdentifier',\n",
       "       'flood_floodZoneMapType', 'flood_stateFIPS',\n",
       "       'flood_floodZoneBaseFloodElevationFeet', 'flood_floodZone',\n",
       "       'flood_communityNumber', 'flood_communityStatus',\n",
       "       'flood_mapEffectiveDate', 'flood_floodHazardBoundaryMapInitialDate',\n",
       "       'flood_floodInsuranceRateMapInitialDate',\n",
       "       'flood_addressLocationElevationFeet',\n",
       "       'flood_year100FloodZoneDistanceFeet',\n",
       "       'flood_year500FloodZoneDistanceFeet',\n",
       "       'flood_elevationProfileToClosestWaterbodyFeet',\n",
       "       'flood_distanceToNearestWaterbodyFeet', 'PBKEY', 'ADD_NUMBER',\n",
       "       'STREETNAME', 'CITY', 'STATE', 'ZIPCODE', 'PLUS4', 'LOC_CODE', 'GEOID',\n",
       "       'LAT', 'LON', 'PROP_TYPE', 'FIPS', 'LivingSquareFootage',\n",
       "       'BedroomCount', 'BathroomCount', 'SaleAmount', 'ParcelID', 'ParcelArea',\n",
       "       'Elevation', 'Geometry', 'BuildingID', 'MaxElevation', 'MinElevation',\n",
       "       'BuildingArea'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features by selecting relevant columns and separating numerical and categorical columns.\n",
    "    \"\"\"\n",
    "    # Remove ID columns and other unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        'psyte_preciselyID', 'coastal_preciselyID', 'flood_preciselyID', 'flood_floodID',\n",
    "        'PBKEY', 'ParcelID', 'BuildingID', 'Geometry', 'GEOID', 'FIPS',\n",
    "        'ADD_NUMBER', 'STREETNAME', 'CITY', 'STATE', 'ZIPCODE', 'PLUS4'\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "    \n",
    "    # Separate numerical and categorical columns\n",
    "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    return df, numerical_columns, categorical_columns\n",
    "\n",
    "def create_preprocessing_pipeline(numerical_columns, categorical_columns):\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline that handles both numerical and categorical data.\n",
    "    \"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_columns),\n",
    "            ('cat', categorical_transformer, categorical_columns)\n",
    "        ])\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def perform_pca_analysis(X_transformed, n_components=0.95):\n",
    "    \"\"\"\n",
    "    Perform PCA and return transformed data along with explained variance ratio.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_transformed)\n",
    "    \n",
    "    # Calculate cumulative explained variance\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    return X_pca, pca, cumulative_variance_ratio\n",
    "\n",
    "def determine_optimal_clusters(X_pca, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Determine optimal number of clusters using elbow method and silhouette score.\n",
    "    \"\"\"\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X_pca)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_pca, kmeans.labels_))\n",
    "    \n",
    "    return range(2, max_clusters + 1), inertias, silhouette_scores\n",
    "\n",
    "def plot_cluster_analysis(n_clusters, inertias, silhouette_scores):\n",
    "    \"\"\"\n",
    "    Plot elbow curve and silhouette scores.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Elbow curve\n",
    "    ax1.plot(n_clusters, inertias, 'bo-')\n",
    "    ax1.set_xlabel('Number of Clusters (k)')\n",
    "    ax1.set_ylabel('Inertia')\n",
    "    ax1.set_title('Elbow Method')\n",
    "    \n",
    "    # Silhouette scores\n",
    "    ax2.plot(n_clusters, silhouette_scores, 'ro-')\n",
    "    ax2.set_xlabel('Number of Clusters (k)')\n",
    "    ax2.set_ylabel('Silhouette Score')\n",
    "    ax2.set_title('Silhouette Analysis')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def perform_clustering_analysis(df):\n",
    "    \"\"\"\n",
    "    Main function to perform the complete clustering analysis.\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    df, numerical_columns, categorical_columns = prepare_features(df)\n",
    "    \n",
    "    # Create and fit preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(numerical_columns, categorical_columns)\n",
    "    X_transformed = preprocessor.fit_transform(df)\n",
    "    \n",
    "    # Perform PCA\n",
    "    X_pca, pca, cumulative_variance_ratio = perform_pca_analysis(X_transformed)\n",
    "    \n",
    "    # Determine optimal number of clusters\n",
    "    n_clusters, inertias, silhouette_scores = determine_optimal_clusters(X_pca)\n",
    "    \n",
    "    # Plot cluster analysis\n",
    "    fig = plot_cluster_analysis(n_clusters, inertias, silhouette_scores)\n",
    "    \n",
    "    # Find optimal number of clusters\n",
    "    optimal_k = n_clusters[np.argmax(silhouette_scores)]\n",
    "    \n",
    "    # Perform final clustering with optimal k\n",
    "    final_kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    cluster_labels = final_kmeans.fit_predict(X_pca)\n",
    "    \n",
    "    return {\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'pca': pca,\n",
    "        'cumulative_variance_ratio': cumulative_variance_ratio,\n",
    "        'optimal_k': optimal_k,\n",
    "        'silhouette_scores': silhouette_scores,\n",
    "        'preprocessor': preprocessor,\n",
    "        'X_pca': X_pca\n",
    "    }\n",
    "# Perform clustering analysis\n",
    "results = perform_clustering_analysis(df)\n",
    "\n",
    "# Access results\n",
    "cluster_labels = results['cluster_labels']\n",
    "optimal_k = results['optimal_k']\n",
    "X_pca = results['X_pca']\n",
    "\n",
    "# Visualize first two PCA components with cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.colorbar(scatter)\n",
    "plt.xlabel('First PCA Component')\n",
    "plt.ylabel('Second PCA Component')\n",
    "plt.title(f'Clustering Results (k={optimal_k})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
